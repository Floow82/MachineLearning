# -*- coding: utf-8 -*-
"""Code_Step1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14V2vJC-6ut7wY0qYZvLFw7vRtkF_jwDI
"""
#---------------------------------------------------IMPORTATIONS----------------------------------------------------------------------------------------
import os
import torch
import torchvision
import pandas as pd
import torch.nn as nn
from torchvision.transforms import transforms
from torchvision.io import read_image
from torch.utils.data import Dataset, Subset, DataLoader
import matplotlib.pyplot as plt
import numpy as np
import torch.nn.functional as F
import gdown, zipfile
from sklearn.model_selection import train_test_split

#---------------------------------------------------CUSTOM DATASET----------------------------------------------------------------------------------------
class FrfDataset(Dataset):
  def __init__ (self, csv_file, root_dir, transform=None, target_transform=None):
    super().__init__()
    self.coord = pd.read_csv(csv_file, sep=';') #read coordinate data from a CSV file
    self.freq = root_dir #Frequency data root directory
    self.transform = transform
    self.target_transform = target_transform

  def __len__(self):
    return len(self.coord)

  def __getitem__(self, idx):

    #Read frequency data
    freq_path = os.path.join(self.freq, self.coord.iloc[idx,0]) #Association of the coordinates and the frequency data
    frequency = pd.read_csv(freq_path, sep=';', skiprows = 1)
    frequency = frequency.iloc[:, [2,3,5,6]] #Only reading columns 2, 3, 5 and 6
    frequency = frequency.apply(lambda x: x.str.replace(',', '.')).apply(pd.to_numeric, errors='coerce').abs()
    frequency = frequency.values.astype(np.float32) #Conversion to numpy float 32
    frequency = frequency.T #from [3001,1] to [1, 3001]
    frequency = np.expand_dims(frequency, axis=0)
    frequency = torch.from_numpy(frequency) #Convert to torch tensor
    frequency = torch.squeeze(frequency, dim=0)

    #Read coordinates data
    coordinate = self.coord.loc[idx, ['x', 'y', 'z']].values.astype(np.float32)  # Cartesian

    if self.transform:
      frequency = self.transform(frequency)
    if self.target_transform:
      coordinate = self.target_transform(coordinate)
    return frequency, coordinate


#---------------------------------------------------DATA DOWNLOADING----------------------------------------------------------------------------------------
url='https://www.dl.dropboxusercontent.com/scl/fi/fmemcxq3mri5d67ep0nxy/step1.zip?rlkey=t4027ydw8mq4lybxz40brtnz1&dl=0'
path='/Users/florineteulieres/Downloads/data'
download_path = f'{path}/step'
if not os.path.exists(path):
  os.makedirs(path)
gdown.download(url, download_path, quiet=False) #Downloadinf from the link as .zip

with zipfile.ZipFile(download_path, 'r') as ziphandler:
  ziphandler.extractall(path) #Extract the folder from the .zip

###Creation of the dataset
data_path = "/Users/florineteulieres/Downloads/data/step1/freq"
csv_file = "/Users/florineteulieres/Downloads/data/step1/coordinates.csv"
ds = FrfDataset(csv_file, data_path)
batch_size = 1

# 80% for the training, 20 % for the testing
train_size = 0.8
train_dataset, test_dataset = train_test_split(ds, train_size=train_size, random_state=42)

# checking the length of each dataset
print("Nombre de donnees d'entrainement:", len(train_dataset))
print("Nombre de donnees de test:", len(test_dataset))

###Creattion of the dataloaders
train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)


#---------------------------------------------------CNN MODEL----------------------------------------------------------------------------------------
class ExcitationLocationModel(nn.Module):
  def __init__(self):
    super().__init__()
    self.conv2 = nn.Conv1d(4,8, kernel_size=16)
    self.pool = nn.MaxPool1d(kernel_size = 4)
    self.conv3 = nn.Conv1d(8,16, kernel_size=16)
    self.avg_pool = nn.AdaptiveAvgPool1d(1)
    self.dropout = nn.Dropout(p = 0.5)
    self.fc = nn.Linear(16,3) #Fully connected layer, from features matrix to coordinates

  def forward(self, x):
    x = F.relu(self.conv2(x))
    x = self.pool(x)
    x = F.relu(self.conv3(x))
    x = self.avg_pool(x)
    x = self.dropout(x)
    x = x.view(x.size(0), -1) #Making the good size for the linear layer
    x = self.fc(x)
    return x



#---------------------------------------------------TRAINING TOOLS----------------------------------------------------------------------------------------
#Definition of the device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

#Instantiate the model and move it to the device (GPU if available)
model = ExcitationLocationModel()
model = model.to(device)
model.float()

#Define the loss function, optimizer, and number of epochs
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)
num_epochs = 1000


#---------------------------------------------------TRAINING LOOP----------------------------------------------------------------------------------------
#Empty list for recording loss values
running_loss_history = []
test_loss_history = []

for epoch in range(num_epochs):
    model.train() # Set the model to training mode
    running_loss = 0.0 #Initialise the loss to 0
    for inputs, labels in train_dataloader:
        freq = inputs.to(device)
        coord= labels.to(device)

        optimizer.zero_grad() #Gradient to zero
        outputs = model(freq)
        loss = criterion(outputs, coord)
        loss.backward() #calculating gradients using the function of loss values
        optimizer.step() #Parameters otpimization

        running_loss += loss.item() #adding the loss value to the list

    # Calculate average training loss for the epoch
    train_loss = running_loss / len(train_dataloader)

    model.eval() # Set the model to evaluation mode (no dropout)
    running_loss = 0.0
    with torch.no_grad():
        for inputs, labels in test_dataloader:
            inputs = inputs.to(device)
            labels = labels.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            running_loss += loss.item()

    #Calculate average test loss for the epoch
    test_loss = running_loss / len(test_dataloader)
    test_loss_history.append(test_loss)
    running_loss_history.append(train_loss)

    # Print the results for the current epoch
    print(f"Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss:.4f}- Test Loss: {test_loss:.4f}")

#---------------------------------------------------VISUALISATION OF THE LOSS HISTORY----------------------------------------------------------------------------------------
plt.plot(running_loss_history, label='training')
plt.plot(test_loss_history, label='test')
plt.legend()


#Number of parameters
print("nombre total de param:", sum(p.numel() for p in model.parameters()))